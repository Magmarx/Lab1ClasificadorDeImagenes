# -*- coding: utf-8 -*-
"""Laboratorio1SP1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQ8Bz276nHKHYr1Oz3ftplPg44tnPq-p

# Imports (En esta secci칩n vamos a importar todas las librerias que precisaremos)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf

from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras import backend as back

import numpy as np
import PIL.Image as Image
import matplotlib.pylab as plt

# Colocamos la seed para poder replicar los experimentos m치s delante
seed=1998
np.random.seed(seed)
tf.random.set_seed(seed)



"""# Variables Globales"""

batch_size = 128
num_classes = 10
epochs = 50
img_rows, img_cols = 28, 28
class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

"""# Dataset prep (En esta secci칩n vamos a preparar el dataset para que tenga el formato optimo para clasificar)

## Loading the dataset
"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_test_demo = x_test
y_test_demo = y_test

plt.imshow(x_train[1])
plt.axis('off')
plt.show()

plt.figure()
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
plt.show()

if back.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

print('train_images shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

"""# Building the model (En esta secci칩n vamos a configurar nuestro dataset de manera que obtengamos un resultado mayor al 70% de accuracy)"""

# Model 1
# acc:  0.8384 (12 Epochs)
# acc: 0.9541 (200 Epochs)
model = Sequential([
        Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
])

# Model 2
# acc:  0.7384
# model = Sequential([
#         Conv2D(34, kernel_size=(5, 5),
#                  activation='relu',
#                  input_shape=input_shape),
#         MaxPooling2D(pool_size=(2, 2)),
#         Conv2D(64, (5, 5), activation='relu'),
#         MaxPooling2D(pool_size=(2, 2)),
#         Dropout(0.25),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
# ])

# Model 3
# acc:  0.70
# model = Sequential([
#         Conv2D(32, kernel_size=(5, 5),
#                  activation='relu',
#                  input_shape=input_shape),
#         MaxPooling2D(pool_size=(2, 2)),
#         Conv2D(6, (5, 5), activation='relu'),
#         MaxPooling2D(pool_size=(2, 2)),
#         Flatten(),
#         Dense(1024, activation='relu'),
#         Dropout(0.4),
#         Dense(num_classes, activation='softmax')
# ])

model.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adadelta(),
              metrics=['accuracy'])

hst = model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))



"""# Eval"""

acc = hst.history['accuracy']
val_acc = hst.history['val_accuracy']

loss = hst.history['loss']
val_loss = hst.history['val_loss']

plt.figure(figsize=(10, 10))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Cross Entropy')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=3)

print('Loss:', test_loss)
print('Accuracy:', test_acc)



"""# Make Predictions"""

predictions = model.predict(x_test)

def plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array, true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array, true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

import random

i = random.randrange(0, 10001, 1)
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], y_test_demo, x_test_demo)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  y_test_demo)
plt.show()

